# **Visual Question Answering en ImÃ¡genes HistopatolÃ³gicas**

### **Proyecto Final â€“ Procesamiento de Texto e ImÃ¡genes con Deep Learning**

---

## **DescripciÃ³n General**

Este repositorio contiene la implementaciÃ³n del proyecto final del curso **Procesamiento de Texto e ImÃ¡genes con Deep Learning**, cuyo objetivo es desarrollar un sistema moderno de **Visual Question Answering (VQA)** aplicado a **imÃ¡genes histopatolÃ³gicas**.

El proyecto utiliza modelos multimodales recientes de visiÃ³nâ€“lenguaje, en particular **LLaVA 1.5 (SigLIP + LLaMA-3) con LoRA**, evaluados sobre el dataset **PathVQA**.

---

## **Objetivo del Proyecto**

Construir y evaluar un sistema multimodal capaz de responder preguntas en lenguaje natural basadas en imÃ¡genes histolÃ³gicas, comparando:

* Un baseline clÃ¡sico (opcional) basado en CLIP + GPT-2.
* Un modelo moderno VLM: **LLaVA 1.5**.

---

## **Dataset**

**PathVQA** (HuggingFace):

* ~5,000 imÃ¡genes histopatolÃ³gicas
* 32,799 pares Preguntaâ€“Respuesta
* Preguntas: SÃ­/No, What/Where/How, hallazgos diagnÃ³sticos, etc.

Link: [https://huggingface.co/datasets/flaviagiammarino/path-vqa](https://huggingface.co/datasets/flaviagiammarino/path-vqa)

---

## **Modelo Principal**

El modelo seleccionado es **LLaVA 1.5**, compuesto por:

* **SigLIP** como encoder visual
* **MLP** como mÃ³dulo de fusiÃ³n visiÃ³nâ†’lenguaje
* **LLaMA-3** como modelo generador
* **LoRA** para fine-tuning eficiente

Pipeline general:

```
Imagen â†’ SigLIP â†’ MLP multimodal â†’ LLaMA-3 â†’ Respuesta generada
```

---

## **MÃ©tricas**

Se emplean mÃ©tricas separadas por tipo de pregunta:

### Para SÃ­/No:

* Accuracy
* F1-score (macro)

### Para preguntas abiertas:

* BLEU
* CIDEr
* (Opcional) ROUGE-L, BERTScore

---

## **ğŸ“ Estructura Propuesta del Repositorio**

```bash
multimodal_vqa_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ protocolo_proyecto_vqa.md
â”‚   â””â”€â”€ documentacion.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ evaluation/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ results/
â”œâ”€â”€ README.md
â””â”€â”€ environment.yml
```

* `configs/`: Archivos YAML de configuraciÃ³n.
* `data/`: Datos procesados (PathVQA).
* `checkpoints/`: Pesos del encoder visual (SigLIP).
* `src/`: CÃ³digo fuente modular.
* `scripts/`: Scripts SLURM para el clÃºster Lab-SB.

---

## **Estado del Proyecto**

El proyecto se encuentra en fase de organizaciÃ³n inicial. PrÃ³ximos pasos:

* Implementar carga de PathVQA.
* Configurar modelo LLaVA 1.5.
* AÃ±adir soporte para LoRA.
* Definir experimentos y mÃ©tricas.

---

## **Autores**

- **Uziel IsaÃ­ LujÃ¡n LÃ³pez**  
- **Diego Paniagua Molina**     

##  Estado

En desarrollo â€“ versiÃ³n inicial del proyecto.  

## Despliegue en ClÃºster (Lab-SB)

### 1. PreparaciÃ³n de Datos (Local)
Los datos y el encoder visual ya estÃ¡n descargados en `data/raw` y `checkpoints/siglip_vision_tower`. Subir la carpeta completa `multimodal_vqa_project`.

### 2. ConfiguraciÃ³n de LLaMA-3
Como el clÃºster no tiene internet, **no intentes descargar LLaMA**.
Edita `configs/train_config.yaml` y cambia la ruta de `llm_model_path` a la ubicaciÃ³n absoluta de los pesos en el clÃºster.

```yaml
paths:
  # Ejemplo:
  llm_model_path: "/home/est_posgrado_uziel.lujan/modelos/llama3-8b-hf"