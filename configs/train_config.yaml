project:
  name: "vqa_tinyllama_clip_fft"
  output_dir: "results/tinyllama-clip-fft"
  logging_dir: "logs"

paths:
  data_dir: "data/raw/path_vqa_hf" 
  
  # RUTAS ACTUALIZADAS (La corrección vital)
  llm_model_path: "checkpoints/TinyLlama-1.1B-Chat"
  vision_tower_path: "checkpoints/clip-vit-large-patch14-336" 

training:
  # Configuración para GPU (Aprovechando tu nodo g-0-6)
  batch_size: 2             # TinyLlama + CLIP caben bien con batch 2 en ~16GB VRAM
  grad_accumulation: 4      # Batch efectivo = 8. Este parametro ayuda a simular batches más grandes
  epochs: 3
  learning_rate: 2.0e-5     # FFT requiere learning rate más bajo que LoRA
  
  # Optimizaciones
  fp16: false               # Usar bf16 si la GPU es Ampere (A100/A10/3090), si no fp16: true
  bf16: true                # Recomendado para estabilidad numérica en entrenamiento
  max_length: 768          # Longitud máxima de secuencia (ajustar según necesidad)          
  gradient_checkpointing: false # Vital para ahorrar VRAM en FFT

lora: # (Esta sección ya no se usa en el código, pero no estorba si se queda)
  r: 128
  alpha: 256
  dropout: 0.05