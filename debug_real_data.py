import torch
from transformers import AutoProcessor, AutoTokenizer, LlavaProcessor
from src.data.dataset import PathVQADataset
from src.utils.paths import get_path
import matplotlib.pyplot as plt

# 1. Configuraci√≥n R√°pida
# Apuntamos a tus carpetas locales reales
DATA_DIR = "./data/raw/path_vqa_hf" 
VISION_PATH = "./checkpoints/siglip_vision_tower"
# Usamos un modelo dummy para el tokenizer si no tienes LLaMA en local,
# o usamos la ruta real si la tienes. Si no, el procesador de SigLIP basta para la imagen.
# Para este test, intentaremos cargar el procesador de visi√≥n puro.

print("üöÄ Iniciando Test de Datos Reales (PathVQA)...")

try:
    # Cargamos solo la parte de visi√≥n para no necesitar LLaMA pesado
    print(f"üìÇ Cargando Image Processor desde: {VISION_PATH}")
    image_processor = AutoProcessor.from_pretrained(VISION_PATH).image_processor
    
    # Creamos un tokenizer "fake" o b√°sico solo para que la clase Dataset no falle
    # (PathVQADataset espera un 'processor' que tenga .tokenizer y .image_processor)
    print("üîß Creando Processor H√≠brido Mock para el test...")
    
    class MockTokenizer:
        def __call__(self, text, **kwargs):
            # Simula tokenizaci√≥n
            return type('obj', (object,), {
                'input_ids': torch.randint(0, 1000, (1, 32)),
                'attention_mask': torch.ones((1, 32))
            })()
        def decode(self, token_ids, **kwargs):
            return "[TEXTO DECODIFICADO SIMULADO]"
            
        pad_token_id = 0
        eos_token_id = 1

    # Clase wrapper para simular el LlavaProcessor completo
    class MockLlavaProcessor:
        def __init__(self, image_processor):
            self.image_processor = image_processor
            self.tokenizer = MockTokenizer()
            
        def __call__(self, text=None, images=None, **kwargs):
            # Esta es la magia: Usamos el procesador REAL de imagen
            pixel_values = self.image_processor(images, return_tensors="pt").pixel_values
            
            # Simulaci√≥n de texto
            return type('obj', (object,), {
                'input_ids': torch.randint(0, 100, (1, 10)),
                'attention_mask': torch.ones((1, 10)),
                'pixel_values': pixel_values
            })()

    processor = MockLlavaProcessor(image_processor)

except Exception as e:
    print(f"‚ùå Error inicializando procesadores: {e}")
    exit()

# 2. Instanciar el Dataset REAL
print(f"üìÇ Cargando Dataset desde disco: {DATA_DIR}")
try:
    # Instanciamos tu clase real
    ds = PathVQADataset(data_path=DATA_DIR, processor=processor, split='train')
    print(f"‚úÖ Dataset cargado. Total de muestras: {len(ds)}")
except Exception as e:
    print(f"‚ùå Error cargando dataset: {e}")
    print("‚ö†Ô∏è  Aseg√∫rate de haber ejecutado 'setup_data.py' o el notebook de descarga primero.")
    exit()

# 3. Inspecci√≥n Profunda de Muestras
print("\nüîé Inspeccionando las primeras 5 muestras...")
print("-" * 60)

for i in range(5):
    # Accedemos al dato crudo primero para ver tama√±o original
    raw_item = ds.dataset[i]
    original_img = raw_item['image']
    original_size = original_img.size # (W, H)
    
    # Ahora pasamos por el __getitem__ del dataset que usa el procesador
    processed_item = ds[i]
    tensor_shape = processed_item['pixel_values'].shape
    
    print(f"Muestra #{i}:")
    print(f"   - Pregunta: {raw_item['question'][:50]}...")
    print(f"   - Tama√±o Original (PIL): {original_size} (Ancho x Alto)")
    print(f"   - Tensor Resultante:     {tensor_shape}  <-- ¬øEs [3, 384, 384]?")
    
    # Validaci√≥n autom√°tica
    if tensor_shape == torch.Size([3, 384, 384]):
        print("   ‚úÖ STATUS: Correcto (Redimensionado OK)")
    else:
        print("   ‚ùå STATUS: Fallo de dimensi√≥n")
    
    print("-" * 60)

print("\nüéâ Conclusi√≥n del Test:")
print("Si viste ‚úÖ en todas las muestras, el procesador est√° manejando")
print("autom√°ticamente la variabilidad de tama√±os de PathVQA.")